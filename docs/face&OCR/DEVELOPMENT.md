# ğŸ›  iPhotron â€” äººè„¸è¯†åˆ« & OCR æ–‡å­—è¯†åˆ« å¼€å‘æ–‡æ¡£

> **ç‰ˆæœ¬**: 1.0.0
> **æ—¥æœŸ**: 2026-02-13
> **çŠ¶æ€**: Draft
> **æ¨¡å—**: Face Recognition & OCR Indexing

---

## ç›®å½• / Table of Contents

1. [æ–‡ä»¶ç»“æ„ / File Structure](#1-æ–‡ä»¶ç»“æ„--file-structure)
2. [æ¨¡å—ä¾èµ– / Dependencies](#2-æ¨¡å—ä¾èµ–--dependencies)
3. [ä¿¡å·æµ / Signal Flow](#3-ä¿¡å·æµ--signal-flow)
4. [æ•°æ®æµ / Data Flow](#4-æ•°æ®æµ--data-flow)
5. [äººè„¸è¯†åˆ«å¼€å‘æŒ‡å— / Face Recognition Dev Guide](#5-äººè„¸è¯†åˆ«å¼€å‘æŒ‡å—--face-recognition-dev-guide)
   - 5.1 [æ£€æµ‹ä¸åµŒå…¥ / Detection & Embedding](#51-æ£€æµ‹ä¸åµŒå…¥--detection--embedding)
   - 5.2 [èšç±» / Clustering](#52-èšç±»--clustering)
   - 5.3 [èšç±»ç®¡ç†æ“ä½œ / Cluster Management](#53-èšç±»ç®¡ç†æ“ä½œ--cluster-management)
6. [OCR å¼€å‘æŒ‡å— / OCR Dev Guide](#6-ocr-å¼€å‘æŒ‡å—--ocr-dev-guide)
   - 6.1 [æ–‡å­—æ£€æµ‹ä¸è¯†åˆ« / Text Detection & Recognition](#61-æ–‡å­—æ£€æµ‹ä¸è¯†åˆ«--text-detection--recognition)
   - 6.2 [æ–‡å­—æœå›¾ / Text-based Image Search](#62-æ–‡å­—æœå›¾--text-based-image-search)
7. [Worker ä¸é˜Ÿåˆ—å®ç° / Worker & Queue Implementation](#7-worker-ä¸é˜Ÿåˆ—å®ç°--worker--queue-implementation)
8. [CUDA åç«¯é›†æˆ / CUDA Backend Integration](#8-cuda-åç«¯é›†æˆ--cuda-backend-integration)
9. [æµ‹è¯•è®¡åˆ’ / Test Plan](#9-æµ‹è¯•è®¡åˆ’--test-plan)
10. [æµ‹è¯•é›† / Test Dataset](#10-æµ‹è¯•é›†--test-dataset)

---

## 1. æ–‡ä»¶ç»“æ„ / File Structure

ä»¥ä¸‹ä¸ºæ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶ï¼Œéµå¾ªé¡¹ç›®ç°æœ‰ DDD + MVVM åˆ†å±‚æ¶æ„ï¼š

```
src/iPhoto/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ ai/                                  # æ–°å¢ï¼šAI æ ¸å¿ƒç®—æ³•å±‚
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ backend.py                       # CUDA/CPU åç«¯æ¢æµ‹ä¸åˆ‡æ¢
â”‚       â”œâ”€â”€ model_manager.py                 # DNN æ¨¡å‹åŠ è½½/ç¼“å­˜ï¼ˆå•ä¾‹ï¼‰
â”‚       â”œâ”€â”€ face/
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ detector.py                  # YuNet äººè„¸æ£€æµ‹å™¨
â”‚       â”‚   â”œâ”€â”€ recognizer.py                # SFace åµŒå…¥æå–å™¨
â”‚       â”‚   â”œâ”€â”€ aligner.py                   # 5 ç‚¹ä»¿å°„å¯¹é½
â”‚       â”‚   â”œâ”€â”€ quality.py                   # è´¨é‡è¿‡æ»¤ï¼ˆæ¨¡ç³Š/ä¾§è„¸/ä½åˆ†è¾¨ç‡ï¼‰
â”‚       â”‚   â””â”€â”€ clustering.py                # å±‚æ¬¡èšç±» + å¢é‡èšç±»
â”‚       â””â”€â”€ ocr/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ text_detector.py             # DB/EAST æ–‡å­—æ£€æµ‹
â”‚           â”œâ”€â”€ text_recognizer.py           # CRNN æ–‡å­—è¯†åˆ«
â”‚           â””â”€â”€ document_builder.py          # åŒºåŸŸåˆå¹¶ â†’ full_text
â”‚
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ index_store/
â”‚       â”œâ”€â”€ face_repository.py               # æ–°å¢ï¼šface_index.db CRUD
â”‚       â”œâ”€â”€ face_engine.py                   # æ–°å¢ï¼šface DB è¿æ¥ç®¡ç†
â”‚       â”œâ”€â”€ ocr_repository.py                # æ–°å¢ï¼šocr_index.db CRUD
â”‚       â””â”€â”€ ocr_engine.py                    # æ–°å¢ï¼šocr DB è¿æ¥ç®¡ç†
â”‚
â”œâ”€â”€ domain/
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ ai.py                            # æ–°å¢ï¼šFace, Cluster, Person, OcrRegion é¢†åŸŸæ¨¡å‹
â”‚
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ use_cases/
â”‚   â”‚   â”œâ”€â”€ process_faces.py                 # æ–°å¢ï¼šäººè„¸å¤„ç†ç”¨ä¾‹
â”‚   â”‚   â”œâ”€â”€ manage_clusters.py               # æ–°å¢ï¼šèšç±»ç®¡ç†ç”¨ä¾‹ï¼ˆåˆå¹¶/æ‹†åˆ†/ç§»åŠ¨ï¼‰
â”‚   â”‚   â”œâ”€â”€ process_ocr.py                   # æ–°å¢ï¼šOCR å¤„ç†ç”¨ä¾‹
â”‚   â”‚   â””â”€â”€ search_by_text.py                # æ–°å¢ï¼šæ–‡å­—æœå›¾ç”¨ä¾‹
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ face_service.py                  # æ–°å¢ï¼šäººè„¸ä¸šåŠ¡æœåŠ¡
â”‚       â””â”€â”€ ocr_service.py                   # æ–°å¢ï¼šOCR ä¸šåŠ¡æœåŠ¡
â”‚
â”œâ”€â”€ library/
â”‚   â””â”€â”€ workers/
â”‚       â”œâ”€â”€ face_worker.py                   # æ–°å¢ï¼šäººè„¸å¤„ç† QRunnable Worker
â”‚       â”œâ”€â”€ ocr_worker.py                    # æ–°å¢ï¼šOCR å¤„ç† QRunnable Worker
â”‚       â””â”€â”€ ai_scheduler.py                  # æ–°å¢ï¼šæ¬¡é˜Ÿåˆ—å…¬å¹³è°ƒåº¦å™¨
â”‚
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ viewmodels/
â”‚   â”‚   â”œâ”€â”€ people_viewmodel.py              # æ–°å¢ï¼šäººç‰©/èšç±»è§†å›¾æ¨¡å‹
â”‚   â”‚   â””â”€â”€ text_search_viewmodel.py         # æ–°å¢ï¼šæ–‡å­—æœå›¾è§†å›¾æ¨¡å‹
â”‚   â””â”€â”€ ui/
â”‚       â”œâ”€â”€ people_view.py                   # æ–°å¢ï¼šäººç‰©æµè§ˆè§†å›¾
â”‚       â”œâ”€â”€ cluster_detail_view.py           # æ–°å¢ï¼šèšç±»è¯¦æƒ…è§†å›¾
â”‚       â””â”€â”€ text_search_view.py              # æ–°å¢ï¼šæ–‡å­—æœç´¢è§†å›¾
â”‚
â”œâ”€â”€ di/
â”‚   â””â”€â”€ container.py                         # ä¿®æ”¹ï¼šæ³¨å†Œ AI ç›¸å…³æœåŠ¡
â”‚
â””â”€â”€ config.py                                # ä¿®æ”¹ï¼šæ–°å¢ AI ç›¸å…³é…ç½®å¸¸é‡

tests/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ test_face_detector.py
â”‚       â”œâ”€â”€ test_face_recognizer.py
â”‚       â”œâ”€â”€ test_clustering.py
â”‚       â”œâ”€â”€ test_text_detector.py
â”‚       â”œâ”€â”€ test_text_recognizer.py
â”‚       â””â”€â”€ test_backend.py
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ test_face_repository.py
â”‚   â””â”€â”€ test_ocr_repository.py
â”œâ”€â”€ application/
â”‚   â”œâ”€â”€ test_process_faces.py
â”‚   â”œâ”€â”€ test_manage_clusters.py
â”‚   â”œâ”€â”€ test_process_ocr.py
â”‚   â””â”€â”€ test_search_by_text.py
â””â”€â”€ fixtures/
    â””â”€â”€ ai/
        â”œâ”€â”€ faces/                           # æµ‹è¯•ç”¨äººè„¸å›¾ç‰‡
        â”œâ”€â”€ text_images/                     # æµ‹è¯•ç”¨å«æ–‡å­—å›¾ç‰‡
        â””â”€â”€ models/                          # æµ‹è¯•ç”¨å°å‹æ¨¡å‹
```

---

## 2. æ¨¡å—ä¾èµ– / Dependencies

### 2.1 æ–°å¢ä¾èµ–

| åŒ…å | ç‰ˆæœ¬ | ç”¨é€” | å¿…éœ€ |
|------|------|------|------|
| `opencv-python-headless` | â‰¥ 4.10 | äººè„¸æ£€æµ‹/OCRï¼ˆå·²å­˜åœ¨äºé¡¹ç›®ï¼‰ | âœ… |
| `opencv-contrib-python` | â‰¥ 4.10 | æ‰©å±•æ¨¡å—ï¼ˆFaceRecognizerSF ç­‰ï¼‰ | å¯é€‰ |
| `scikit-learn` | â‰¥ 1.4 | AgglomerativeClustering å±‚æ¬¡èšç±» | âœ… |
| `scipy` | â‰¥ 1.12 | ä½™å¼¦è·ç¦»çŸ©é˜µè®¡ç®— | âœ… |

### 2.2 å¯é€‰ä¾èµ–ï¼ˆCUDA åŠ é€Ÿï¼‰

| åŒ…å | è¯´æ˜ |
|------|------|
| `opencv-python-headless` ç¼–è¯‘å¸¦ CUDA | æˆ–ä½¿ç”¨ `opencv-contrib-python` çš„ CUDA æ„å»º |
| NVIDIA CUDA Toolkit â‰¥ 11.8 | GPU é©±åŠ¨æ”¯æŒ |
| cuDNN â‰¥ 8.6 | DNN åŠ é€Ÿåº“ |

### 2.3 ä¾èµ–å…³ç³»å›¾

```mermaid
graph TD
    A[core.ai.face.detector] -->|cv2.FaceDetectorYN| B[OpenCV DNN]
    A2[core.ai.face.recognizer] -->|cv2.FaceRecognizerSF| B
    A3[core.ai.ocr.text_detector] -->|cv2.dnn.readNet| B
    A4[core.ai.ocr.text_recognizer] -->|cv2.dnn.readNet| B
    B -->|å¯é€‰| C[CUDA Backend]
    B -->|é»˜è®¤| D[CPU Backend]
    E[core.ai.face.clustering] --> F[scikit-learn]
    E --> G[scipy.spatial.distance]
    H[core.ai.backend] --> B
    H --> C
```

---

## 3. ä¿¡å·æµ / Signal Flow

### 3.1 äººè„¸å¤„ç†ä¿¡å·æµ

```mermaid
sequenceDiagram
    participant SM as ScannerWorker
    participant BTM as BackgroundTaskManager
    participant AIS as AIScheduler
    participant FW as FaceWorker
    participant FDB as face_index.db
    participant UI as PeopleView

    SM->>BTM: finished(rel_list)
    BTM->>AIS: enqueue_face_batch(rel_list)
    AIS->>FW: submit (rel_batch)

    loop æ¯å¼ å›¾ç‰‡
        FW->>FW: åŠ è½½å›¾ç‰‡
        FW->>FW: YuNet æ£€æµ‹äººè„¸
        FW->>FW: SFace æå–åµŒå…¥
        FW->>FW: è´¨é‡è¿‡æ»¤
        FW->>FDB: INSERT faces
        FW-->>AIS: progressUpdated(current, total)
    end

    FW->>FW: å¢é‡èšç±»ï¼ˆæ–°äººè„¸ vs ç°æœ‰èšç±»ä¸­å¿ƒï¼‰
    FW->>FDB: UPDATE faces SET cluster_id
    FW->>FDB: UPDATE clusters SET centroid
    FW-->>BTM: finished(face_stats)
    BTM-->>UI: facesUpdated()
```

### 3.2 OCR å¤„ç†ä¿¡å·æµ

```mermaid
sequenceDiagram
    participant SM as ScannerWorker
    participant BTM as BackgroundTaskManager
    participant AIS as AIScheduler
    participant OW as OcrWorker
    participant ODB as ocr_index.db
    participant UI as TextSearchView

    SM->>BTM: finished(rel_list)
    BTM->>AIS: enqueue_ocr_batch(rel_list)
    AIS->>OW: submit (rel_batch)

    loop æ¯å¼ å›¾ç‰‡
        OW->>OW: åŠ è½½å›¾ç‰‡
        OW->>OW: DB/EAST æ£€æµ‹æ–‡å­—åŒºåŸŸ
        OW->>OW: CRNN è¯†åˆ«æ–‡å­—
        OW->>OW: æ„å»º full_text
        OW->>ODB: INSERT ocr_regions
        OW->>ODB: UPSERT ocr_documents
        OW-->>AIS: progressUpdated(current, total)
    end

    OW-->>BTM: finished(ocr_stats)
    BTM-->>UI: ocrUpdated()
```

### 3.3 èšç±»ç®¡ç†ä¿¡å·æµ

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant UI as ClusterDetailView
    participant VM as PeopleViewModel
    participant UC as ManageClustersUseCase
    participant FDB as face_index.db

    U->>UI: åˆå¹¶èšç±» A + B
    UI->>VM: merge_clusters([A, B])
    VM->>UC: execute_merge(source_ids, target_id)
    UC->>FDB: UPDATE faces SET cluster_id=target WHERE cluster_id IN (sources)
    UC->>FDB: DELETE FROM clusters WHERE cluster_id IN (sources)
    UC->>FDB: é‡ç®— target centroid
    UC-->>VM: MergeResult
    VM-->>UI: clustersChanged()

    U->>UI: ç§»åŠ¨å•å¼ äººè„¸åˆ°èšç±» C
    UI->>VM: move_face(face_id, target_cluster_id)
    VM->>UC: execute_move(face_id, target_cluster_id)
    UC->>FDB: UPDATE faces SET cluster_id=target WHERE face_id=?
    UC->>FDB: é‡ç®—åŸèšç±»å’Œç›®æ ‡èšç±» centroid
    UC-->>VM: MoveResult
    VM-->>UI: clustersChanged()
```

---

## 4. æ•°æ®æµ / Data Flow

### 4.1 ç«¯åˆ°ç«¯æ•°æ®æµå…¨æ™¯

```mermaid
flowchart LR
    subgraph è¾“å…¥
        IMG[åŸå§‹ç…§ç‰‡æ–‡ä»¶]
    end

    subgraph Primary Queue
        SC[ScannerWorker]
        MDB[(global_index.db)]
        SC -->|EXIF/å…ƒæ•°æ®| MDB
    end

    subgraph Secondary Queues
        direction TB
        subgraph Face Pipeline
            FD[FaceDetector<br/>YuNet]
            FR[FaceRecognizer<br/>SFace]
            CL[Clustering<br/>Agglomerative]
            FDBX[(face_index.db)]
            FD -->|ROI + landmarks| FR
            FR -->|128-D embedding| CL
            CL -->|cluster_id| FDBX
        end
        subgraph OCR Pipeline
            TD[TextDetector<br/>DB/EAST]
            TR[TextRecognizer<br/>CRNN]
            DB2[DocumentBuilder]
            ODBX[(ocr_index.db)]
            TD -->|text regions| TR
            TR -->|text + confidence| DB2
            DB2 -->|full_text| ODBX
        end
    end

    IMG --> SC
    SC -->|rel_list| FD
    SC -->|rel_list| TD
    IMG --> FD
    IMG --> TD

    subgraph è¾“å‡º
        PV[äººç‰©æµè§ˆè§†å›¾]
        TS[æ–‡å­—æœç´¢è§†å›¾]
    end

    FDBX --> PV
    ODBX --> TS
    MDB --> PV
    MDB --> TS
```

### 4.2 äººè„¸åµŒå…¥æ•°æ®æ ¼å¼

```python
# åµŒå…¥å‘é‡å­˜å‚¨æ ¼å¼
import numpy as np

embedding: np.ndarray  # shape=(128,), dtype=np.float32

# åºåˆ—åŒ–åˆ° SQLite BLOB
blob: bytes = embedding.tobytes()           # 512 bytes

# ä» BLOB ååºåˆ—åŒ–
embedding = np.frombuffer(blob, dtype=np.float32)  # shape=(128,)

# ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
from scipy.spatial.distance import cosine
similarity = 1 - cosine(emb_a, emb_b)       # 1.0 = å®Œå…¨ç›¸åŒ
```

### 4.3 èšç±»æ•°æ®æµ

```mermaid
flowchart TD
    A[æœªèšç±»äººè„¸<br/>cluster_id IS NULL] --> B{äººè„¸æ•° < 10000?}
    B -->|æ˜¯| C[å…¨é‡å±‚æ¬¡èšç±»<br/>AgglomerativeClustering]
    B -->|å¦| D[å¢é‡èšç±»]

    C --> E[ç”Ÿæˆ cluster_id + centroid]
    D --> F{ä¸ç°æœ‰ centroid<br/>è·ç¦» < é˜ˆå€¼?}
    F -->|æ˜¯| G[åˆå…¥æœ€è¿‘èšç±»<br/>æ›´æ–° centroid]
    F -->|å¦| H[åˆ›å»ºæ–°èšç±»]

    E --> I[(face_index.db)]
    G --> I
    H --> I

    I --> J[ç”¨æˆ·æ‰‹åŠ¨æ“ä½œ]
    J --> K[åˆå¹¶èšç±»]
    J --> L[ç§»åŠ¨å•å¼ ]
    J --> M[æ‹†åˆ†èšç±»]
    J --> N[å‘½åäººç‰©]

    K --> O[é‡ç®— centroid]
    L --> O
    M --> O
    O --> I
```

---

## 5. äººè„¸è¯†åˆ«å¼€å‘æŒ‡å— / Face Recognition Dev Guide

### 5.1 æ£€æµ‹ä¸åµŒå…¥ / Detection & Embedding

#### 5.1.1 FaceDetector ç±»

```python
# src/iPhoto/core/ai/face/detector.py

class FaceDetector:
    """åŸºäº OpenCV YuNet çš„äººè„¸æ£€æµ‹å™¨"""

    def __init__(self, model_path: Path, backend: DnnBackend):
        self._detector = cv2.FaceDetectorYN.create(
            str(model_path),
            "",
            (320, 320),
            score_threshold=0.7,
            nms_threshold=0.3,
            top_k=5000,
            backend_id=backend.backend_id,
            target_id=backend.target_id,
        )

    def detect(self, image: np.ndarray) -> list[FaceROI]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„äººè„¸ã€‚

        Args:
            image: BGR æ ¼å¼çš„ numpy æ•°ç»„

        Returns:
            FaceROI åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«å½’ä¸€åŒ– bboxã€confidenceã€landmarks
        """
        h, w = image.shape[:2]
        self._detector.setInputSize((w, h))
        _, faces = self._detector.detect(image)

        if faces is None:
            return []

        results = []
        for face in faces:
            x, y, fw, fh = face[0:4]
            conf = face[-1]
            landmarks = face[4:14].reshape(5, 2)

            results.append(FaceROI(
                bbox_x=x / w, bbox_y=y / h,
                bbox_w=fw / w, bbox_h=fh / h,
                confidence=float(conf),
                landmarks=[(lx / w, ly / h) for lx, ly in landmarks],
            ))
        return results
```

#### 5.1.2 FaceRecognizer ç±»

```python
# src/iPhoto/core/ai/face/recognizer.py

class FaceRecognizer:
    """åŸºäº OpenCV SFace çš„äººè„¸åµŒå…¥æå–å™¨"""

    def __init__(self, model_path: Path, backend: DnnBackend):
        self._recognizer = cv2.FaceRecognizerSF.create(
            str(model_path), "",
            backend_id=backend.backend_id,
            target_id=backend.target_id,
        )

    def extract_embedding(
        self, image: np.ndarray, face_roi: FaceROI
    ) -> np.ndarray:
        """
        æå– 128-D äººè„¸åµŒå…¥å‘é‡ã€‚

        Args:
            image: åŸå§‹ BGR å›¾åƒ
            face_roi: æ£€æµ‹åˆ°çš„äººè„¸åŒºåŸŸ

        Returns:
            L2 å½’ä¸€åŒ–çš„ 128-D float32 å‘é‡
        """
        h, w = image.shape[:2]
        face_array = np.array([
            face_roi.bbox_x * w, face_roi.bbox_y * h,
            face_roi.bbox_w * w, face_roi.bbox_h * h,
            *[c * s for lm in face_roi.landmarks for c, s in zip(lm, (w, h))],
        ], dtype=np.float32)

        aligned = self._recognizer.alignCrop(image, face_array)
        embedding = self._recognizer.feature(aligned)
        return embedding.flatten()
```

#### 5.1.3 è´¨é‡è¿‡æ»¤

```python
# src/iPhoto/core/ai/face/quality.py

class QualityFilter:
    """äººè„¸è´¨é‡è¯„ä¼°"""

    MIN_SIZE = 48        # æœ€å°äººè„¸å°ºå¯¸ï¼ˆåƒç´ ï¼‰
    BLUR_THRESHOLD = 100  # æ‹‰æ™®æ‹‰æ–¯æ–¹å·®é˜ˆå€¼
    YAW_THRESHOLD = 45    # åè½¬è§’é˜ˆå€¼ï¼ˆåº¦ï¼‰

    def evaluate(
        self, image: np.ndarray, roi: FaceROI
    ) -> list[str]:
        """
        è¿”å›è´¨é‡æ ‡è®°åˆ—è¡¨ã€‚

        Returns:
            å¦‚ ['low_quality', 'blurry'] æˆ–ç©ºåˆ—è¡¨ï¼ˆè´¨é‡åˆæ ¼ï¼‰
        """
        flags = []
        h, w = image.shape[:2]
        face_w = roi.bbox_w * w
        face_h = roi.bbox_h * h

        if face_w < self.MIN_SIZE or face_h < self.MIN_SIZE:
            flags.append("low_quality")

        # æ¨¡ç³Šæ£€æµ‹
        x1, y1 = int(roi.bbox_x * w), int(roi.bbox_y * h)
        x2, y2 = int(x1 + face_w), int(y1 + face_h)
        face_crop = image[y1:y2, x1:x2]
        if face_crop.size > 0:
            gray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)
            variance = cv2.Laplacian(gray, cv2.CV_64F).var()
            if variance < self.BLUR_THRESHOLD:
                flags.append("blurry")

        # ä¾§è„¸æ£€æµ‹ï¼ˆåŸºäºå…³é”®ç‚¹ï¼‰
        if roi.landmarks and len(roi.landmarks) >= 3:
            left_eye, right_eye, nose = roi.landmarks[:3]
            eye_center_x = (left_eye[0] + right_eye[0]) / 2
            nose_x = nose[0]
            eye_dist = abs(right_eye[0] - left_eye[0])
            if eye_dist > 0:
                yaw_ratio = abs(nose_x - eye_center_x) / eye_dist
                if yaw_ratio > 0.5:  # è¿‘ä¼¼ >45 åº¦
                    flags.append("side_face")

        return flags
```

---

### 5.2 èšç±» / Clustering

#### 5.2.1 å…¨é‡èšç±»

```python
# src/iPhoto/core/ai/face/clustering.py

from sklearn.cluster import AgglomerativeClustering
from scipy.spatial.distance import cosine
import numpy as np
import uuid

class FaceClusterer:
    """äººè„¸èšç±»å¼•æ“"""

    FULL_CLUSTER_THRESHOLD = 10_000
    DEFAULT_DISTANCE_THRESHOLD = 0.40

    def cluster_full(
        self,
        embeddings: np.ndarray,
        distance_threshold: float = DEFAULT_DISTANCE_THRESHOLD,
    ) -> np.ndarray:
        """
        å…¨é‡å±‚æ¬¡èšç±»ã€‚

        Args:
            embeddings: shape (N, 128) çš„åµŒå…¥çŸ©é˜µ
            distance_threshold: èšç±»è·ç¦»é˜ˆå€¼

        Returns:
            shape (N,) çš„èšç±»æ ‡ç­¾æ•°ç»„
        """
        if len(embeddings) < 2:
            return np.zeros(len(embeddings), dtype=int)

        clustering = AgglomerativeClustering(
            n_clusters=None,
            metric="cosine",
            linkage="average",
            distance_threshold=distance_threshold,
        )
        return clustering.fit_predict(embeddings)

    def cluster_incremental(
        self,
        new_embeddings: np.ndarray,
        existing_centroids: dict[str, np.ndarray],
        distance_threshold: float = DEFAULT_DISTANCE_THRESHOLD,
    ) -> dict[int, str]:
        """
        å¢é‡èšç±»ï¼šæ–°åµŒå…¥ä¸ç°æœ‰ä¸­å¿ƒæ¯”è¾ƒã€‚

        Args:
            new_embeddings: shape (M, 128)
            existing_centroids: {cluster_id: centroid_vector}
            distance_threshold: è·ç¦»é˜ˆå€¼

        Returns:
            {æ–°åµŒå…¥ç´¢å¼•: cluster_id} æ˜ å°„ï¼ŒæœªåŒ¹é…çš„å€¼ä¸ºæ–°ç”Ÿæˆçš„ UUID
        """
        assignments: dict[int, str] = {}
        centroid_ids = list(existing_centroids.keys())
        centroid_matrix = np.array(list(existing_centroids.values()))

        for i, emb in enumerate(new_embeddings):
            if len(centroid_matrix) == 0:
                new_id = str(uuid.uuid4())
                assignments[i] = new_id
                centroid_ids.append(new_id)
                centroid_matrix = (
                    np.vstack([centroid_matrix, emb.reshape(1, -1)])
                    if centroid_matrix.size else emb.reshape(1, -1)
                )
                continue

            distances = [cosine(emb, c) for c in centroid_matrix]
            min_idx = int(np.argmin(distances))
            min_dist = distances[min_idx]

            if min_dist < distance_threshold:
                assignments[i] = centroid_ids[min_idx]
            else:
                new_id = str(uuid.uuid4())
                assignments[i] = new_id
                centroid_ids.append(new_id)
                centroid_matrix = np.vstack(
                    [centroid_matrix, emb.reshape(1, -1)]
                )

        return assignments

    @staticmethod
    def compute_centroid(embeddings: np.ndarray) -> np.ndarray:
        """è®¡ç®—èšç±»ä¸­å¿ƒå‘é‡ï¼ˆL2 å½’ä¸€åŒ–çš„å‡å€¼ï¼‰"""
        centroid = embeddings.mean(axis=0)
        norm = np.linalg.norm(centroid)
        if norm > 0:
            centroid /= norm
        return centroid
```

---

### 5.3 èšç±»ç®¡ç†æ“ä½œ / Cluster Management

#### 5.3.1 åˆå¹¶èšç±»

```python
# src/iPhoto/application/use_cases/manage_clusters.pyï¼ˆéƒ¨åˆ†ï¼‰

class MergeClustersUseCase:
    """åˆå¹¶ä¸¤ä¸ªæˆ–å¤šä¸ªèšç±»"""

    def __init__(self, face_repo: FaceRepository):
        self._repo = face_repo

    def execute(
        self, source_cluster_ids: list[str], target_cluster_id: str
    ) -> MergeResult:
        """
        å°† source èšç±»ä¸­çš„æ‰€æœ‰äººè„¸åˆå¹¶åˆ° target èšç±»ã€‚

        æ­¥éª¤:
        1. æ›´æ–°æ‰€æœ‰ source äººè„¸çš„ cluster_id â†’ target
        2. åˆ é™¤ source clusters è®°å½•
        3. é‡ç®— target èšç±»ä¸­å¿ƒå‘é‡
        4. æ›´æ–° target face_count
        """
        with self._repo.transaction():
            # ç§»åŠ¨äººè„¸
            for src_id in source_cluster_ids:
                if src_id == target_cluster_id:
                    continue
                self._repo.reassign_faces(src_id, target_cluster_id)
                self._repo.delete_cluster(src_id)

            # é‡ç®—ä¸­å¿ƒå‘é‡
            embeddings = self._repo.get_cluster_embeddings(target_cluster_id)
            new_centroid = FaceClusterer.compute_centroid(embeddings)
            face_count = len(embeddings)
            self._repo.update_cluster_centroid(
                target_cluster_id, new_centroid, face_count
            )

        return MergeResult(
            target_cluster_id=target_cluster_id,
            merged_count=len(source_cluster_ids) - 1,
            total_faces=face_count,
        )
```

#### 5.3.2 ç§»åŠ¨å•å¼ äººè„¸

```python
class MoveFaceUseCase:
    """ç§»åŠ¨å•å¼ äººè„¸åˆ°ç›®æ ‡èšç±»"""

    def __init__(self, face_repo: FaceRepository):
        self._repo = face_repo

    def execute(
        self, face_id: str, target_cluster_id: str
    ) -> MoveResult:
        """
        æ­¥éª¤:
        1. è·å–äººè„¸å½“å‰ cluster_id
        2. æ›´æ–°äººè„¸ cluster_id â†’ target
        3. é‡ç®—åŸèšç±»ä¸­å¿ƒå‘é‡å’Œ face_count
        4. é‡ç®—ç›®æ ‡èšç±»ä¸­å¿ƒå‘é‡å’Œ face_count
        5. å¦‚æœåŸèšç±»å˜ç©ºï¼Œæ ‡è®°ä¸º deleted
        """
        face = self._repo.get_face(face_id)
        old_cluster_id = face.cluster_id

        with self._repo.transaction():
            self._repo.update_face_cluster(face_id, target_cluster_id)

            # é‡ç®—ç›®æ ‡èšç±»
            target_embs = self._repo.get_cluster_embeddings(target_cluster_id)
            self._repo.update_cluster_centroid(
                target_cluster_id,
                FaceClusterer.compute_centroid(target_embs),
                len(target_embs),
            )

            # é‡ç®—åŸèšç±»
            if old_cluster_id:
                old_embs = self._repo.get_cluster_embeddings(old_cluster_id)
                if len(old_embs) == 0:
                    self._repo.mark_cluster_deleted(old_cluster_id)
                else:
                    self._repo.update_cluster_centroid(
                        old_cluster_id,
                        FaceClusterer.compute_centroid(old_embs),
                        len(old_embs),
                    )

        return MoveResult(
            face_id=face_id,
            from_cluster=old_cluster_id,
            to_cluster=target_cluster_id,
        )
```

#### 5.3.3 æ‹†åˆ†èšç±»

```python
class SplitClusterUseCase:
    """å¯¹é€‰ä¸­èšç±»ä½¿ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼é‡æ–°èšç±»"""

    def execute(
        self, cluster_id: str, stricter_threshold: float = 0.25
    ) -> SplitResult:
        """
        æ­¥éª¤:
        1. è·å–èšç±»å†…æ‰€æœ‰äººè„¸åµŒå…¥
        2. ä½¿ç”¨æ›´ä¸¥æ ¼é˜ˆå€¼é‡æ–°è¿è¡Œå±‚æ¬¡èšç±»
        3. ä¸ºæ–°å­èšç±»åˆ›å»ºè®°å½•
        4. æ›´æ–°äººè„¸ cluster_id
        5. æ ‡è®°åŸèšç±»ä¸º deleted
        """
        embeddings = self._repo.get_cluster_embeddings(cluster_id)
        face_ids = self._repo.get_cluster_face_ids(cluster_id)

        labels = self._clusterer.cluster_full(
            embeddings, distance_threshold=stricter_threshold
        )

        new_clusters = {}
        with self._repo.transaction():
            for label in set(labels):
                new_id = str(uuid.uuid4())
                mask = labels == label
                cluster_embs = embeddings[mask]
                centroid = FaceClusterer.compute_centroid(cluster_embs)
                self._repo.create_cluster(new_id, centroid, int(mask.sum()))
                new_clusters[label] = new_id

                for face_id in np.array(face_ids)[mask]:
                    self._repo.update_face_cluster(face_id, new_id)

            self._repo.mark_cluster_deleted(cluster_id)

        return SplitResult(
            original_cluster_id=cluster_id,
            new_cluster_ids=list(new_clusters.values()),
        )
```

---

## 6. OCR å¼€å‘æŒ‡å— / OCR Dev Guide

### 6.1 æ–‡å­—æ£€æµ‹ä¸è¯†åˆ« / Text Detection & Recognition

#### 6.1.1 TextDetector ç±»

```python
# src/iPhoto/core/ai/ocr/text_detector.py

class TextDetector:
    """åŸºäº OpenCV DNN çš„æ–‡å­—æ£€æµ‹å™¨ï¼ˆDB æ¨¡å‹ï¼‰"""

    def __init__(self, model_path: Path, backend: DnnBackend):
        self._net = cv2.dnn.readNet(str(model_path))
        self._net.setPreferableBackend(backend.backend_id)
        self._net.setPreferableTarget(backend.target_id)
        self._confidence_threshold = 0.5

    def detect(self, image: np.ndarray) -> list[TextRegion]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„æ–‡å­—åŒºåŸŸã€‚

        Returns:
            TextRegion åˆ—è¡¨ï¼ŒåŒ…å«å½’ä¸€åŒ– bbox å’Œæ—‹è½¬è§’åº¦
        """
        h, w = image.shape[:2]
        blob = cv2.dnn.blobFromImage(
            image, 1.0 / 255.0, (736, 736),
            mean=(122.67891434, 116.66876762, 104.00698793),
        )
        self._net.setInput(blob)
        output = self._net.forward()

        # åå¤„ç†ï¼šäºŒå€¼åŒ– â†’ è½®å»“æ£€æµ‹ â†’ æœ€å°å¤–æ¥æ—‹è½¬çŸ©å½¢
        regions = self._postprocess(output, w, h)
        return [r for r in regions if r.confidence >= self._confidence_threshold]
```

#### 6.1.2 TextRecognizer ç±»

```python
# src/iPhoto/core/ai/ocr/text_recognizer.py

class TextRecognizer:
    """åŸºäº CRNN çš„æ–‡å­—è¯†åˆ«å™¨"""

    def __init__(self, model_path: Path, backend: DnnBackend):
        self._net = cv2.dnn.readNet(str(model_path))
        self._net.setPreferableBackend(backend.backend_id)
        self._net.setPreferableTarget(backend.target_id)

    def recognize(
        self, image: np.ndarray, region: TextRegion
    ) -> RecognitionResult:
        """
        è¯†åˆ«æ–‡å­—åŒºåŸŸä¸­çš„å†…å®¹ã€‚

        Returns:
            RecognitionResult(text, confidence, lang)
        """
        # è£å‰ªå¹¶é€è§†å˜æ¢æ–‡å­—åŒºåŸŸ
        cropped = self._crop_region(image, region)
        # é¢„å¤„ç†: ç°åº¦ â†’ ç¼©æ”¾åˆ°å›ºå®šé«˜åº¦ 32px
        processed = self._preprocess(cropped)

        blob = cv2.dnn.blobFromImage(processed)
        self._net.setInput(blob)
        output = self._net.forward()

        # CTC è§£ç 
        text, confidence = self._ctc_decode(output)
        lang = self._detect_language(text)

        return RecognitionResult(
            text=text,
            confidence=confidence,
            lang=lang,
        )
```

#### 6.1.3 DocumentBuilder

```python
# src/iPhoto/core/ai/ocr/document_builder.py

class DocumentBuilder:
    """å°†å¤šä¸ªæ–‡å­—åŒºåŸŸåˆå¹¶ä¸ºå®Œæ•´æ–‡æ¡£"""

    def build(self, regions: list[RecognizedRegion]) -> Document:
        """
        æŒ‰é˜…è¯»é¡ºåºï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³ï¼‰åˆå¹¶æ–‡å­—åŒºåŸŸã€‚

        æ’åºè§„åˆ™:
        1. æŒ‰ y åæ ‡åˆ†ç»„ï¼ˆy å·®å€¼ < é˜ˆå€¼è§†ä¸ºåŒè¡Œï¼‰
        2. åŒè¡Œå†…æŒ‰ x åæ ‡æ’åº
        3. è¡Œé—´ç”¨æ¢è¡Œç¬¦åˆ†éš”ï¼ŒåŒè¡Œç”¨ç©ºæ ¼åˆ†éš”
        """
        if not regions:
            return Document(full_text="", region_count=0)

        # æŒ‰ y åæ ‡æ’åºå¹¶åˆ†è¡Œ
        sorted_regions = sorted(regions, key=lambda r: (r.bbox_y, r.bbox_x))
        lines: list[list[RecognizedRegion]] = []
        current_line: list[RecognizedRegion] = [sorted_regions[0]]

        for region in sorted_regions[1:]:
            if abs(region.bbox_y - current_line[0].bbox_y) < 0.02:
                current_line.append(region)
            else:
                lines.append(sorted(current_line, key=lambda r: r.bbox_x))
                current_line = [region]
        lines.append(sorted(current_line, key=lambda r: r.bbox_x))

        full_text = "\n".join(
            " ".join(r.text for r in line) for line in lines
        )
        confidences = [r.confidence for r in regions]
        primary_lang = self._majority_lang(regions)

        return Document(
            full_text=full_text,
            region_count=len(regions),
            avg_confidence=sum(confidences) / len(confidences),
            primary_lang=primary_lang,
        )
```

### 6.2 æ–‡å­—æœå›¾ / Text-based Image Search

#### 6.2.1 æœç´¢ç”¨ä¾‹

```python
# src/iPhoto/application/use_cases/search_by_text.py

class SearchByTextUseCase:
    """æ ¹æ®æ–‡å­—å†…å®¹æœç´¢å›¾ç‰‡"""

    def __init__(self, ocr_repo: OcrRepository, asset_repo: AssetRepository):
        self._ocr_repo = ocr_repo
        self._asset_repo = asset_repo

    def execute(self, query: str, limit: int = 50) -> list[SearchResult]:
        """
        æ‰§è¡Œæ–‡å­—æœç´¢ã€‚

        ç­–ç•¥:
        1. ä¼˜å…ˆä½¿ç”¨ FTS5 å…¨æ–‡æœç´¢ï¼ˆç²¾ç¡®åŒ¹é… + åˆ†è¯ï¼‰
        2. FTS5 æ— ç»“æœæ—¶å›é€€åˆ° LIKE æ¨¡ç³ŠåŒ¹é…
        3. ç»“æœé€šè¿‡ rel å…³è”ä¸»åº“è·å–èµ„äº§ä¿¡æ¯
        """
        # FTS5 æœç´¢
        fts_results = self._ocr_repo.search_fts(query, limit)

        if not fts_results:
            # å›é€€åˆ° LIKE
            fts_results = self._ocr_repo.search_like(query, limit)

        if not fts_results:
            return []

        # å…³è”ä¸»åº“è·å–èµ„äº§å…ƒæ•°æ®
        rels = [r.rel for r in fts_results]
        assets = self._asset_repo.get_assets_by_rels(rels)
        asset_map = {a["rel"]: a for a in assets}

        return [
            SearchResult(
                rel=r.rel,
                matched_text=r.snippet,
                confidence=r.avg_confidence,
                asset=asset_map.get(r.rel),
            )
            for r in fts_results
            if r.rel in asset_map
        ]
```

#### 6.2.2 OcrRepository æœç´¢æ–¹æ³•

```python
# src/iPhoto/cache/index_store/ocr_repository.pyï¼ˆéƒ¨åˆ†ï¼‰

class OcrRepository:
    def search_fts(self, query: str, limit: int = 50) -> list[FtsMatch]:
        """FTS5 å…¨æ–‡æœç´¢"""
        sql = """
            SELECT d.rel,
                   snippet(ocr_fts, 0, '<<', '>>', '...', 32) AS snippet,
                   d.avg_confidence,
                   rank
            FROM ocr_fts fts
            JOIN ocr_documents d ON fts.rowid = d.rowid
            WHERE ocr_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = self._execute(sql, (query, limit))
        return [FtsMatch(**r) for r in rows]

    def search_like(self, query: str, limit: int = 50) -> list[FtsMatch]:
        """LIKE æ¨¡ç³Šæœç´¢ï¼ˆFTS5 å›é€€æ–¹æ¡ˆï¼‰"""
        sql = """
            SELECT rel, full_text AS snippet, avg_confidence, 0 AS rank
            FROM ocr_documents
            WHERE full_text LIKE ?
            ORDER BY avg_confidence DESC
            LIMIT ?
        """
        rows = self._execute(sql, (f"%{query}%", limit))
        return [FtsMatch(**r) for r in rows]
```

---

## 7. Worker ä¸é˜Ÿåˆ—å®ç° / Worker & Queue Implementation

### 7.1 FaceWorker

```python
# src/iPhoto/library/workers/face_worker.py

class FaceWorkerSignals(QObject):
    progressUpdated = Signal(int, int)        # (current, total)
    chunkReady = Signal(list)                 # [face_dict, ...]
    finished = Signal(dict)                   # {face_count, cluster_count, errors}
    error = Signal(str)

class FaceWorker(QRunnable):
    """äººè„¸æ£€æµ‹+åµŒå…¥+å…¥åº“ Worker"""

    BATCH_SIZE = 10

    def __init__(
        self,
        rel_list: list[str],
        library_root: Path,
        signals: FaceWorkerSignals,
    ):
        super().__init__()
        self.setAutoDelete(True)
        self._rels = rel_list
        self._library_root = library_root
        self._signals = signals
        self._is_cancelled = False

    def run(self) -> None:
        try:
            detector = ModelManager.instance().get_face_detector()
            recognizer = ModelManager.instance().get_face_recognizer()
            quality = QualityFilter()
            repo = FaceRepository(self._library_root)
            total = len(self._rels)

            new_faces = []
            for i, rel in enumerate(self._rels):
                if self._is_cancelled:
                    break

                image_path = self._library_root / rel
                if not image_path.exists():
                    repo.log_process(rel, "skipped")
                    continue

                try:
                    image = cv2.imread(str(image_path))
                    if image is None:
                        repo.log_process(rel, "error", error_msg="æ— æ³•è¯»å–å›¾ç‰‡")
                        continue

                    rois = detector.detect(image)
                    for roi in rois:
                        flags = quality.evaluate(image, roi)
                        embedding = None
                        if "low_quality" not in flags:
                            embedding = recognizer.extract_embedding(image, roi)

                        face = repo.insert_face(rel, roi, embedding, flags)
                        if embedding is not None:
                            new_faces.append(face)

                    repo.log_process(rel, "done", face_count=len(rois))

                except Exception as e:
                    repo.log_process(rel, "error", error_msg=str(e))

                self._signals.progressUpdated.emit(i + 1, total)

            # å¢é‡èšç±»
            if new_faces:
                clusterer = FaceClusterer()
                existing = repo.get_all_centroids()
                embeddings = np.array([f.embedding for f in new_faces])
                assignments = clusterer.cluster_incremental(
                    embeddings, existing
                )
                repo.apply_cluster_assignments(new_faces, assignments)

            self._signals.finished.emit({
                "face_count": len(new_faces),
                "processed": total,
            })

        except Exception as e:
            self._signals.error.emit(str(e))

    def cancel(self) -> None:
        self._is_cancelled = True
```

### 7.2 OcrWorker

```python
# src/iPhoto/library/workers/ocr_worker.py

class OcrWorkerSignals(QObject):
    progressUpdated = Signal(int, int)
    finished = Signal(dict)
    error = Signal(str)

class OcrWorker(QRunnable):
    """OCR æ–‡å­—è¯†åˆ«+å…¥åº“ Worker"""

    def __init__(
        self,
        rel_list: list[str],
        library_root: Path,
        signals: OcrWorkerSignals,
    ):
        super().__init__()
        self.setAutoDelete(True)
        self._rels = rel_list
        self._library_root = library_root
        self._signals = signals
        self._is_cancelled = False

    def run(self) -> None:
        try:
            text_detector = ModelManager.instance().get_text_detector()
            text_recognizer = ModelManager.instance().get_text_recognizer()
            doc_builder = DocumentBuilder()
            repo = OcrRepository(self._library_root)
            total = len(self._rels)

            for i, rel in enumerate(self._rels):
                if self._is_cancelled:
                    break

                image_path = self._library_root / rel
                if not image_path.exists():
                    repo.log_process(rel, "skipped")
                    continue

                try:
                    image = cv2.imread(str(image_path))
                    if image is None:
                        repo.log_process(rel, "error", error_msg="æ— æ³•è¯»å–å›¾ç‰‡")
                        continue

                    regions = text_detector.detect(image)
                    recognized = []
                    for region in regions:
                        result = text_recognizer.recognize(image, region)
                        recognized.append(RecognizedRegion(
                            region=region, text=result.text,
                            confidence=result.confidence, lang=result.lang,
                        ))

                    repo.insert_regions(rel, recognized)

                    if recognized:
                        doc = doc_builder.build(recognized)
                        repo.upsert_document(rel, doc)

                    repo.log_process(
                        rel, "done", region_count=len(recognized)
                    )

                except Exception as e:
                    repo.log_process(rel, "error", error_msg=str(e))

                self._signals.progressUpdated.emit(i + 1, total)

            self._signals.finished.emit({"processed": total})

        except Exception as e:
            self._signals.error.emit(str(e))

    def cancel(self) -> None:
        self._is_cancelled = True
```

### 7.3 AIScheduler â€” å…¬å¹³è°ƒåº¦å™¨

```python
# src/iPhoto/library/workers/ai_scheduler.py

import threading
from collections import deque
from PySide6.QtCore import QThreadPool, QObject, Signal

class AIScheduler(QObject):
    """
    æ¬¡é˜Ÿåˆ—å…¬å¹³è°ƒåº¦å™¨ã€‚

    èŒè´£:
    - ç»´æŠ¤ Face å’Œ OCR ä¸¤ä¸ª FIFO é˜Ÿåˆ—
    - åŠ æƒè½®è¯¢ (WRR) åˆ†é…å…±äº«çº¿ç¨‹
    - èƒŒå‹æ§åˆ¶ï¼ˆé˜Ÿåˆ—é•¿åº¦ä¸Šé™ï¼‰
    - èµ„æºç›‘æ§ï¼ˆå†…å­˜ä½¿ç”¨ç‡ï¼‰
    """

    MAX_QUEUE_LENGTH = 1000
    MEMORY_PAUSE_THRESHOLD = 0.80
    MEMORY_RESUME_THRESHOLD = 0.70

    queueStatus = Signal(str, int)  # (queue_name, pending_count)

    def __init__(self, thread_pool: QThreadPool, max_workers: int = 2):
        super().__init__()
        self._pool = thread_pool
        self._max_workers = max_workers
        self._active_workers = 0
        self._lock = threading.Lock()

        self._face_queue: deque[list[str]] = deque()
        self._ocr_queue: deque[list[str]] = deque()
        self._face_weight = 1
        self._ocr_weight = 1
        self._rr_counter = 0

    def enqueue_face_batch(self, rels: list[str]) -> None:
        """å°†ä¸€æ‰¹ rel åŠ å…¥äººè„¸å¤„ç†é˜Ÿåˆ—"""
        with self._lock:
            if len(self._face_queue) >= self.MAX_QUEUE_LENGTH:
                return  # èƒŒå‹: ä¸¢å¼ƒ
            self._face_queue.append(rels)
            self.queueStatus.emit("face", len(self._face_queue))
        self._try_dispatch()

    def enqueue_ocr_batch(self, rels: list[str]) -> None:
        """å°†ä¸€æ‰¹ rel åŠ å…¥ OCR å¤„ç†é˜Ÿåˆ—"""
        with self._lock:
            if len(self._ocr_queue) >= self.MAX_QUEUE_LENGTH:
                return
            self._ocr_queue.append(rels)
            self.queueStatus.emit("ocr", len(self._ocr_queue))
        self._try_dispatch()

    def _try_dispatch(self) -> None:
        """å°è¯•ä»é˜Ÿåˆ—ä¸­å–å‡ºä»»åŠ¡å¹¶æäº¤åˆ°çº¿ç¨‹æ± """
        with self._lock:
            while self._active_workers < self._max_workers:
                batch, queue_type = self._pick_next()
                if batch is None:
                    break
                self._active_workers += 1
                self._submit_worker(batch, queue_type)

    def _pick_next(self) -> tuple[list[str] | None, str]:
        """åŠ æƒè½®è¯¢é€‰æ‹©ä¸‹ä¸€ä¸ªé˜Ÿåˆ—"""
        self._adjust_weights()

        for _ in range(self._face_weight + self._ocr_weight):
            self._rr_counter += 1
            if self._rr_counter <= self._face_weight:
                if self._face_queue:
                    return self._face_queue.popleft(), "face"
            else:
                if self._ocr_queue:
                    return self._ocr_queue.popleft(), "ocr"

            if self._rr_counter >= self._face_weight + self._ocr_weight:
                self._rr_counter = 0

        return None, ""

    def _adjust_weights(self) -> None:
        """æ ¹æ®ç§¯å‹é‡åŠ¨æ€è°ƒæ•´æƒé‡"""
        fl, ol = len(self._face_queue), len(self._ocr_queue)
        if fl > ol * 2:
            self._face_weight, self._ocr_weight = 2, 1
        elif ol > fl * 2:
            self._face_weight, self._ocr_weight = 1, 2
        else:
            self._face_weight, self._ocr_weight = 1, 1

    def _on_worker_finished(self, queue_type: str) -> None:
        """Worker å®Œæˆåå›è°ƒ"""
        with self._lock:
            self._active_workers -= 1
        self._try_dispatch()
```

---

## 8. CUDA åç«¯é›†æˆ / CUDA Backend Integration

```python
# src/iPhoto/core/ai/backend.py

import cv2
import logging

logger = logging.getLogger(__name__)

class DnnBackend:
    """DNN æ¨ç†åç«¯é…ç½®"""

    def __init__(self, backend_id: int, target_id: int, name: str):
        self.backend_id = backend_id
        self.target_id = target_id
        self.name = name

    def __repr__(self) -> str:
        return f"DnnBackend({self.name})"


_cached_backend: DnnBackend | None = None


def detect_backend() -> DnnBackend:
    """
    æ¢æµ‹æœ€ä¼˜ DNN åç«¯ã€‚

    ä¼˜å…ˆçº§: CUDA > CPU
    ç»“æœç¼“å­˜ä¸ºå•ä¾‹ã€‚
    """
    global _cached_backend
    if _cached_backend is not None:
        return _cached_backend

    try:
        if cv2.cuda.getCudaEnabledDeviceCount() > 0:
            _cached_backend = DnnBackend(
                cv2.dnn.DNN_BACKEND_CUDA,
                cv2.dnn.DNN_TARGET_CUDA,
                "CUDA",
            )
            logger.info("CUDA backend detected, using GPU acceleration")
        else:
            raise RuntimeError("No CUDA device")
    except Exception:
        _cached_backend = DnnBackend(
            cv2.dnn.DNN_BACKEND_OPENCV,
            cv2.dnn.DNN_TARGET_CPU,
            "CPU",
        )
        logger.info("CUDA not available, falling back to CPU backend")

    return _cached_backend


def get_cpu_fallback() -> DnnBackend:
    """è·å– CPU å›é€€åç«¯ï¼ˆç”¨äº GPU OOM ç­‰å¼‚å¸¸æƒ…å†µï¼‰"""
    return DnnBackend(
        cv2.dnn.DNN_BACKEND_OPENCV,
        cv2.dnn.DNN_TARGET_CPU,
        "CPU (fallback)",
    )
```

### 8.1 GPU äº’æ–¥é”

```python
# src/iPhoto/core/ai/backend.pyï¼ˆç»­ï¼‰

import threading

_gpu_lock = threading.Lock()
_GPU_TIMEOUT = 30  # ç§’


def acquire_gpu(timeout: float = _GPU_TIMEOUT) -> bool:
    """
    å°è¯•è·å– GPU äº’æ–¥é”ã€‚

    Returns:
        True è·å–æˆåŠŸï¼ŒFalse è¶…æ—¶
    """
    return _gpu_lock.acquire(timeout=timeout)


def release_gpu() -> None:
    """é‡Šæ”¾ GPU äº’æ–¥é”"""
    try:
        _gpu_lock.release()
    except RuntimeError:
        pass  # æœªæŒæœ‰é”æ—¶å¿½ç•¥
```

---

## 9. æµ‹è¯•è®¡åˆ’ / Test Plan

### 9.1 å•å…ƒæµ‹è¯•

| æµ‹è¯•æ–‡ä»¶ | æµ‹è¯•å¯¹è±¡ | å…³é”®ç”¨ä¾‹ |
|---------|---------|---------|
| `test_face_detector.py` | `FaceDetector` | æ— äººè„¸å›¾ç‰‡è¿”å›ç©ºåˆ—è¡¨ï¼›å¤šäººè„¸å›¾ç‰‡è¿”å›æ­£ç¡®æ•°é‡ï¼›bbox å½’ä¸€åŒ–å€¼åŸŸ [0,1] |
| `test_face_recognizer.py` | `FaceRecognizer` | åµŒå…¥å‘é‡ shape=(128,)ï¼›åŒä¸€äººä¸¤å¼ ç…§ç‰‡ä½™å¼¦è·ç¦» < 0.3ï¼›ä¸åŒäºº > 0.5 |
| `test_clustering.py` | `FaceClusterer` | å…¨é‡èšç±»ï¼š2 äºº 10 å¼ æ­£ç¡®åˆ†ä¸º 2 ç»„ï¼›å¢é‡èšç±»ï¼šæ–°äººè„¸åŒ¹é…å·²æœ‰èšç±»ï¼›é˜ˆå€¼è°ƒæ•´å½±å“èšç±»æ•° |
| `test_quality.py` | `QualityFilter` | å°äººè„¸æ ‡è®° `low_quality`ï¼›æ¨¡ç³Šäººè„¸æ ‡è®° `blurry`ï¼›ä¾§è„¸æ ‡è®° `side_face` |
| `test_text_detector.py` | `TextDetector` | æœ‰æ–‡å­—å›¾ç‰‡æ£€æµ‹åˆ°åŒºåŸŸï¼›æ— æ–‡å­—å›¾ç‰‡è¿”å›ç©ºåˆ—è¡¨ï¼›ä¸­è‹±æ–‡å‡å¯æ£€æµ‹ |
| `test_text_recognizer.py` | `TextRecognizer` | è‹±æ–‡è¯†åˆ«å‡†ç¡®ç‡ > 80%ï¼›ä¸­æ–‡è¯†åˆ«å‡†ç¡®ç‡ > 70%ï¼›ç½®ä¿¡åº¦èŒƒå›´ [0,1] |
| `test_document_builder.py` | `DocumentBuilder` | é˜…è¯»é¡ºåºæ­£ç¡®ï¼ˆä»ä¸Šåˆ°ä¸‹ï¼Œä»å·¦åˆ°å³ï¼‰ï¼›ç©ºåŒºåŸŸè¿”å›ç©ºæ–‡æ¡£ |
| `test_backend.py` | `detect_backend` | æ—  CUDA è¿”å› CPU åç«¯ï¼›ç»“æœç¼“å­˜ï¼ˆç¬¬äºŒæ¬¡è°ƒç”¨ä¸é‡æ–°æ¢æµ‹ï¼‰ |

### 9.2 é›†æˆæµ‹è¯•

| æµ‹è¯•æ–‡ä»¶ | æµ‹è¯•å¯¹è±¡ | å…³é”®ç”¨ä¾‹ |
|---------|---------|---------|
| `test_face_repository.py` | `FaceRepository` | æ’å…¥/æŸ¥è¯¢/æ›´æ–°äººè„¸è®°å½•ï¼›èšç±»åˆ†é…æŒä¹…åŒ–ï¼›process_log è®°å½• |
| `test_ocr_repository.py` | `OcrRepository` | æ’å…¥/æŸ¥è¯¢æ–‡å­—åŒºåŸŸï¼›FTS5 å…¨æ–‡æœç´¢ï¼›LIKE å›é€€æœç´¢ |
| `test_process_faces.py` | `FaceWorker` ç«¯åˆ°ç«¯ | ä¸€æ‰¹å›¾ç‰‡ â†’ äººè„¸æ£€æµ‹ â†’ åµŒå…¥ â†’ å…¥åº“ â†’ å¢é‡èšç±» |
| `test_process_ocr.py` | `OcrWorker` ç«¯åˆ°ç«¯ | ä¸€æ‰¹å›¾ç‰‡ â†’ æ–‡å­—æ£€æµ‹ â†’ è¯†åˆ« â†’ full_text å…¥åº“ |
| `test_manage_clusters.py` | èšç±»ç®¡ç†ç”¨ä¾‹ | åˆå¹¶ä¸¤ä¸ªèšç±» â†’ éªŒè¯ centroid æ›´æ–°ï¼›ç§»åŠ¨å•å¼  â†’ éªŒè¯åŒå‘ centroid é‡ç®— |
| `test_search_by_text.py` | `SearchByTextUseCase` | FTS5 å‘½ä¸­ â†’ è¿”å›å…³è”èµ„äº§ï¼›æ—  FTS5 å‘½ä¸­ â†’ LIKE å›é€€ï¼›ç©ºæŸ¥è¯¢è¿”å›ç©º |

### 9.3 æ€§èƒ½æµ‹è¯•

| åœºæ™¯ | æ•°æ®é‡ | ç›®æ ‡æŒ‡æ ‡ |
|------|--------|---------|
| äººè„¸æ£€æµ‹åå (CPU) | 100 å¼  2000px å›¾ç‰‡ | â‰¥ 2 å¼ /ç§’ |
| äººè„¸æ£€æµ‹åå (GPU) | 100 å¼  2000px å›¾ç‰‡ | â‰¥ 5 å¼ /ç§’ |
| å…¨é‡èšç±» | 10,000 ä¸ª 128-D å‘é‡ | â‰¤ 30 ç§’ |
| FTS5 æœç´¢å»¶è¿Ÿ | 100,000 æ–‡æ¡£åº“ | â‰¤ 50ms |
| å¢é‡èšç±» | 100 æ–°å‘é‡ vs 10,000 ç°æœ‰ | â‰¤ 2 ç§’ |

---

## 10. æµ‹è¯•é›† / Test Dataset

### 10.1 äººè„¸æµ‹è¯•é›†

| æ•°æ®é›† | è¯´æ˜ | ç”¨é€” |
|--------|------|------|
| `fixtures/ai/faces/single/` | 5 å¼ ä»…å« 1 ä¸ªäººè„¸çš„å›¾ç‰‡ | åŸºç¡€æ£€æµ‹æµ‹è¯• |
| `fixtures/ai/faces/multi/` | 5 å¼ å« 2-5 ä¸ªäººè„¸çš„å›¾ç‰‡ | å¤šäººè„¸æ£€æµ‹ |
| `fixtures/ai/faces/no_face/` | 5 å¼ æ— äººè„¸çš„å›¾ç‰‡ï¼ˆé£æ™¯/ç‰©å“ï¼‰ | ç©ºç»“æœæµ‹è¯• |
| `fixtures/ai/faces/person_A/` | åŒä¸€äºº 10 å¼ ä¸åŒè§’åº¦/å…‰ç…§çš„ç…§ç‰‡ | èšç±»æ­£ç¡®æ€§ |
| `fixtures/ai/faces/person_B/` | å¦ä¸€äºº 10 å¼ ç…§ç‰‡ | èšç±»åŒºåˆ†åº¦ |
| `fixtures/ai/faces/edge_cases/` | æ¨¡ç³Šã€ä¾§è„¸ã€ä½åˆ†è¾¨ç‡ã€é®æŒ¡ | è´¨é‡è¿‡æ»¤æµ‹è¯• |

### 10.2 OCR æµ‹è¯•é›†

| æ•°æ®é›† | è¯´æ˜ | ç”¨é€” |
|--------|------|------|
| `fixtures/ai/text_images/english/` | 5 å¼ å«è‹±æ–‡æ–‡å­—çš„å›¾ç‰‡ | è‹±æ–‡è¯†åˆ« |
| `fixtures/ai/text_images/chinese/` | 5 å¼ å«ä¸­æ–‡æ–‡å­—çš„å›¾ç‰‡ | ä¸­æ–‡è¯†åˆ« |
| `fixtures/ai/text_images/mixed/` | 5 å¼ ä¸­è‹±æ··åˆæ–‡å­—å›¾ç‰‡ | æ··åˆè¯­è¨€ |
| `fixtures/ai/text_images/no_text/` | 5 å¼ æ— æ–‡å­—å›¾ç‰‡ | ç©ºç»“æœæµ‹è¯• |
| `fixtures/ai/text_images/dense/` | å«å¤§é‡æ–‡å­—çš„æˆªå›¾/æ–‡æ¡£ç…§ç‰‡ | é˜…è¯»é¡ºåºæµ‹è¯• |
| `fixtures/ai/text_images/rotated/` | æ—‹è½¬/å€¾æ–œæ–‡å­— | æ—‹è½¬çŸ©å½¢æ£€æµ‹ |

### 10.3 æµ‹è¯•æ•°æ®è¦æ±‚

- æ‰€æœ‰æµ‹è¯•å›¾ç‰‡é¡»ä¸º**å…ç‰ˆæƒ**æˆ–é¡¹ç›®è‡ªè¡Œæ‹æ‘„
- äººè„¸æµ‹è¯•å›¾ç‰‡é¡»ç¡®ä¿**æ— éšç§é£é™©**ï¼ˆå»ºè®®ä½¿ç”¨ AI ç”Ÿæˆçš„äººè„¸æˆ–å…¬å¼€æ•°æ®é›†å¦‚ LFW çš„å­é›†ï¼‰
- å›¾ç‰‡å°ºå¯¸è¦†ç›–ï¼š`640Ã—480`ï¼ˆä½åˆ†è¾¨ç‡ï¼‰ã€`1920Ã—1080`ï¼ˆæ ‡å‡†ï¼‰ã€`4000Ã—3000`ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰
- æ ¼å¼è¦†ç›–ï¼šJPEGã€PNGã€HEICï¼ˆé€šè¿‡ pillow-heif æ”¯æŒï¼‰
